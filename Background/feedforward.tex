\section{Feedforward Neural Network}
Feedforward neural network is a network of which unit in the graph does not form a cycle~\cite{deeplearning-book}.

Each feedforward neural network consists at least one input layer and one output layer.
Each layer contains one or more neurons (network nodes).
Each neuron consists of a linear activation or nonlinear activation function.

Each node has an activation function. There are two type of activation function: Linear and nonlinear.

Example of linear function: \\

\begin{equation} \label{eq:linearmlp}
{f(x) = Wx + b }
\end{equation}

Example of nonlinear function: \\
\[f(x) = tanh(x) \]
\[f(x) =\frac{1}{1+e^{-x}} \]

\subsection{Single-layer perceptron}
Single-layer perceptron is a feedforward neural network which has only one input layer and one output layer of neuron unit (See fig \ref{fig:singleperceptron}). Single-layer perceptron cannot solve some problem such as XOR.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{figure/singleperceptron}
	\caption[Single-layer Perceptron]{Single-layer Perceptron}
	\label{fig:singleperceptron}
\end{figure}



\subsection{Multi-layer perceptron}
Multilayer perceptron (MLP) is a feedforward neural network consist of 2 or more layers (See fig \ref{fig:multilayerperceptron}).
First layer is called input layer.
Last layer is called output layer.
All layers between input and output layer are hidden layers.
A series of linear function is equal to one layer layer.
Hence, MLP always contains nonlinear activation function.
In MLP, every unit of previous layers connects to all units in next layers.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{figure/multilayerperceptron}
	\caption[Multi-layer Perceptron]{Multi-layer Perceptron}
	\label{fig:multilayerperceptron}
\end{figure}

\subsection{Forward pass}
Forward pass, or forward propagation, is a process which information flows from input x through the network to output y, then produces a scalar value cost $J(\theta)$.
In MLP forward pass, an input x is fed through first layer, also known as input layer.
Output from first layer is used as input for second layer, and so on until it reaches last layer, output layer. Output from output layer are output of MLP.

For example: a MLP with 4 layers: $f(x)$, $g(x)$, $h(x)$, $k(x)$. With $f(x)$ is input layer, $k(x)$ is output layer, and $g(x), h(x)$ are hidden layers .With input $x$, then MLP has output $ y = k(h(g(f(x))))$.

\subsection{Cost function}
Cost function $J(\theta)$  is a measurement the error between the prediction $\hat y$ and the ground truth $y$.
When our model predicts exactly the labeled output, then the cost is zero. The problem is to minimize the cost function.
Cost function of MLP depends on output layer.

For most of our experiment, we would use softmax output layer and cross-entropy loss function.

\subsection{Backward pass}
Backward pass, or backpropagation~\cite{rumelhart1988learning}, is a process which information flow from cost $J(\theta)$ back to $x$ in other to compute gradient of desired variables.

Backpropagation applies chain rule to compute gradient of $\nabla_xf(x, y)$, with x is variable desired compute gradient and y is not required gradient.
Given $y = g(x)$ and $z = f(y)$, the chain rule states:

\begin{equation} \label{eq:chainrule}
{\frac  {dz}{dx}}={\frac  {dz}{dy}}\cdot {\frac  {dy}{dx}}
\end{equation}

Table \ref{table:gradient} is an example of computing gradient using chain rule.
\begin{table}[]
	\centering
	\caption{Compute gradient of x using chain rule}
	\label{table:gradient}
	\begin{tabular}{|l|}
		\hline
		Compute gradient of x                                                               \\ \hline
		\begin{tabular}[c]{@{}l@{}}Given \\ $y = x + 3$ \\ $z = 6y^{2}$\end{tabular}                  \\ \hline
		${\frac {dz}{dx}}={\frac {dz}{dy}}\cdot {\frac {dy}{dx}}$                                     \\
		${\frac {dz}{dx}} = (6y^{2})' (x+3)'$                                                         \\
		${\frac {dz}{dx}} = (6(x+3)^{2})' (x+3)'$                                                     \\
		${\frac {dz}{dx}} = 12(x+3)$                                                                  \\ \hline
		\begin{tabular}[c]{@{}l@{}}Given x = 2\\ $\frac{dz}{dx}\bigr\rvert_{x_i=2} = 60$\end{tabular} \\ \hline
	\end{tabular}
\end{table}

In backpropagation step, ${\frac {\partial J}{\partial w}}$ (with $w$ are model's parameters) are computed.
Then, learning algorithm (i.e: AdaGrad, Adam, ...) use gradient to update parameters. One trivial way to update parameter is :
\begin{equation} \label{eq:updated}
{ w \leftarrow w - \alpha {\frac {\partial J}{\partial w}}}
\end{equation}

In MLP, gradient of linear layer \ref{eq:linearmlp} parameters such as weight $\frac {\partial J}{\partial w}$ and bias $\frac {\partial J}{\partial b}$ are computed.
