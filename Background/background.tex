\hypertarget{chap:background}{\chapter{Background}}\label{chap:background}
\section{Sentiment Analysis}
In the nutshell, sentiment analysis is to determine whether the opinion about a specific product, event, organization is positive or negative.
\subsection{The need for sentiment analysis}
Every business needs feedback from customer. Feedback help company know their product strength and weakness, drive business strategy. For example, fix the flaw in product, target new customer segment or halt a campaign. Traditional, company would survey their customer to collect feedback.  The feedback collected from direct survey are limited for many reasons, such as low population.

With the growth of social network and e-commerce, more and more people post their opinion online via blog, social network (Facebook, Twitter), e-commerce site (Amazon, eBay). Companies want to take advantage of these data. Thus, automated systems to analysis opinion are in demand.
\subsection{Different levels in sentiment analysis}
\subsubsection{Document-level}
Document-level sentiment analysis is to determine whether a document (usually a full review) about a specific entity (a product, location, service, ...) is positive or negative. For example,~\cite{pang2002thumbs} perform document-level sentiment analysis on movies review data.
\subsubsection{Sentence-level}\label{sec:sent-level}
Sentence-level sentiment analysis is to determine whether a sentence expressed positive, negative. In this thesis, we focus our study on sentence-level sentiment analysis~\cite{liu2012sentiment}.
\subsubsection{Aspect-level}
Aspect-level  sentiment analysis purpose is to determine opinion, whether positive or negative, against specific aspect of entity~\cite{liu2012sentiment}.

\input{Background/feedforward}

\section{Parse tree in NLP}
Parse tree is a syntactic representation of a sentence. In this thesis, we work on two type of parse tree, Constituency Parse Tree, and Dependency Parse Tree.
\subsection{Constituency Parse Tree}
Constituency Parser breaks sentences into smaller phases. The root node represents a sentence. Children node represent a phrase or word constitutes parent node. Each leaf node contains a word or a punctuation. Each inner node contains a phrase. Each node is Part-of-speech tag labeled. Edges are unlabeled.

Constituency Parse Tree are construct based on Chomsky Phase Structure~\cite{chomsky2002syntactic}. Given a list of rule, derive sentences following the rule. Constituency Parse Tree illustrates the derivation.


% \begin{equation}
% \label{eq:crule}
% \begin{aligned}
% &S \leftarrow NP + VP + .  \\
% &NP \leftarrow PRP  \\
% &NP \leftarrow DT + NN  \\
% &VP \leftarrow VBP + NP\\
% &VRP \leftarrow I \\
% &DT \leftarrow the \\
% &NN \leftarrow cat \\
% &VBP \leftarrow feed \\
% \end{aligned}
% \end{equation}
For example, we have the following rule:
\begin{enumerate}[label=(\roman*)]
    \item $S \leftarrow NP + VP + .$
    \item $NP \leftarrow PRP $
    \item $NP \leftarrow DT + NN$
    \item $VP \leftarrow VBP + NP$
    \item $VRP \leftarrow I$
    \item $DT \leftarrow the$
    \item $NN \leftarrow cat$
    \item $VBP \leftarrow feed$
\end{enumerate}
We apply rule to "I feed the cat". Result of derivation in Table \ref{ifeedmycat}  where roman on right column indicate the rule is use to derive previous statement. The derivation is illustrate in fig \ref{fig:ifeedthecatconstituency}

% \begin{equation}
% \label{eq:catparse}
% \begin{aligned}
% &S \\
% &NP + VP + .\\
% & PRP + VP + . \\
% & PRP + VBP + NP + . \\
% & PRP + VBP + DT + NN + . \\
% & I + VBP + DT + NN + . \\
% & I + feed + DT + NN + . \\
% & I + feed + my + NN + . \\
% & I + feed + my + cat + . \\
% \end{aligned}
% \end{equation}

\begin{table}[H]
	\centering
	\begin{tabular}{ll}
	S	&  \\
	NP + VP + .	& (i) \\
	PRP + VP + .	& (ii) \\
	PRP + VBP + NP + .	& (iv)  \\
	PRP + VBP + DT + NN + .	& (iii) \\
	I + VBP + DT + NN + .	&  (v) \\
	I + feed + DT + NN + .	&  (viii) \\
	I + feed + the + NN + .	&  (vi) \\
	I + feed + the + cat + .	& (vii)
	\end{tabular}
\caption{I feed the cat derivation}
\label{ifeedmycat}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figure/ifeedthecatconstituency}
    \caption[Constituency Parse Tree]{Constituency Parse Tree}
    \label{fig:ifeedthecatconstituency}
\end{figure}

\subsection{Dependency Parse Tree}
Dependency Parse Tree represents dependency relationship of each word in sentences. As oppose to Constituency Parse Tree, each node in Dependency Parse Tree contain one word. Edge are labeled by type of dependency relationship of head word and the dependent. In this thesis, we use Universal Dependencies~\cite{nivre2016universal}. Fig \ref{fig:udexample} is an example of Dependency Parse Tree.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/udexample}
    \caption[Dependency Tree]{Dependency Tree}
    \label{fig:udexample}
\end{figure}


\section{Technical}
\subsection{Anaconda}
Anaconda\footnote{\url{https://www.continuum.io/downloads}} is a Python ecosystem for data science. Anaconda is an all-in-one installation come with many advantage comparing to traditional Python installation.

\subsubsection{All-in-one installation}
Anaconda is install with essential package for machine learning and data science such as Scipy\footnote{\url{https://www.scipy.org/}}, scikit-learn\footnote{\url{http://scikit-learn.org/}}, OpenBLAS\footnote{\url{http://www.openblas.net/}} and package manager tool otherwise have to install manually on traditional Python distro\footnote{\url{https://www.python.org/}}. All pre-install package make installing deeplearning framework easily. All-in-one installation is a most importance feature that affect our choice Anaconda.

\subsubsection{Package and environment manager}  
In traditional Python, people use pip (preferred installer program)\footnote{\url{https://pypi.python.org/pypi/pip}} as package manager to install package and virtualenv\footnote{\url{https://virtualenv.pypa.io/en/stable/}}. Both have to be installed manually.

Anaconda distro come with Conda\footnote{\url{https://conda.io/docs/}}, a package manager and environment manager. Literally, Conda can replace both pip and virtualenv. However, both conda and pip co-exist because some Python package can only be installed from on pip, some others only exist on Conda.


\subsection{Deeplearning Framework}
All deeplearning Framework have some common feature. They are all capable of handle matrices computation efficiently. Recently deeplearning framework can leverage the power of GPU to run faster. Each framework have their own backend implementation. Thus, benchmark of different framework are variety. The difference maybe insignificant in prototype, however .We are making prototype for deeplearning model. Therefore, documentation and language interface were our top criterion for choosing a framework to work on. 

\subsubsection{Torch}\label{sec:torch}
Torch\footnote{http://torch.ch/} is Lua scientific computing framework. Torch support high performing matrix calculation via multi-dimensional array call Tensor. Torch is built with C/C++, CUDA backend. Torch authors chose Lua because Lua works well with C/C++~\cite{collobert2011torch7}.  Thus, Torch is high performing and support GPU. Torch have neural network package (nn) package. Computation graph must be defined before forward pass.

A simple, single linear layer network can be easily defined with few line of code (see listing \ref{lst:torchlinear}).

\begin{lstlisting}[caption={Simple linear layer in Torch},label={lst:torchlinear}, language={[5.1]Lua}]
-- simple y = Ax + b linear layer
l = nn.Linear(2,3)
-- forward pass
x = torch.Tensor(2)
y = l:forward(x) -- vector dimension of 3
\end{lstlisting}

However, when a model need multiple modules, such as multilayer perceptron (MLP), these modules must be put into container. Figure \ref{fig:nncontainer} illustrates on function of each nn container . In order to construct two-layer perception (eq \ref{eq:mlp}), linear, tanh and softmax module must be packed into sequential module (see listing \ref{lst:torchmlp}).

\begin{equation}
\label{eq:mlp}
\begin{aligned}
&h = tanh(W_1*x + b_1) \\
&y = softmax(W_2*h + b2)
\end{aligned}
\end{equation}


\begin{lstlisting}[caption={MLP in Torch},label={lst:torchmlp}, language={[5.1]Lua}]
model = nn.Sequential()
model:add(nn.Linear(2,3))
model:add(nn.Tanh())
model:add(nn.Linear(3,5))
model:add(nn.SoftMax())
-- forward
x = torch.Tensor(2)
y = model:forward(x)
\end{lstlisting}

Torch provide nngraph package support build more complicate model. For example, define MLP in (eq \ref{eq:mlp}) use nngraph (see listing \ref{lst:torchnngraph})

\begin{lstlisting}[caption={MLP using nngraph},label={lst:torchnngraph}, language={[5.1]Lua}]
model = nn.Sequential()
model:add(nn.Linear(2,3))
model:add(nn.Tanh())
model:add(nn.Linear(3,5))
model:add(nn.SoftMax())
-- forward
x = torch.Tensor(2)
y = model:forward(x)
\end{lstlisting}

Sample code on training a model, see Appendix \ref{lst:torchtrain}

\subsubsection{Theano}
Theano\footnote{\url{http://deeplearning.net/software/theano/}} is a deep learning library on Python. It basic function is similar to Torch: matrix calculation, support GPU. Theano is define-and-run scheme, which a computer graph must be built before it is executed.

\begin{lstlisting}[caption={Define function in Theano},label={lst:theanof}, language={python}]
x = T.dmatrix('x')
y = T.dmatrix('y')
z = x + y
f = function([x, y], z)
f([[1, 1], [2, 2]], [[3, 3], [4, 4]])
# result [[4, 4], [6, 6]]
\end{lstlisting}

Comparing to Torch7, Theano is slower on most benchmark~\cite{collobert2011torch7}. Theano does not provide nice template like linear layer. Thus, model must be defined from equation. It gives researcher more control over mathematics aspect but causes more trouble for beginner. A sample code for MLP \ref{lst:theanomlp}. One more problem is that the 'define-and-run' scheme does not suitable for recursive neural network due to recompiling the computation graph each training sample take time.

There are a library called Keras~\cite{chollet2017keras}, a high-level deep learning interface that work on top of Theano, TensorFlow, CNTK. Keras make mathematics in Theano simple for beginner and saves time for experienced user.

\subsubsection{Pytorch}\label{sec:pytorch}
PyTorch uses the same backend as Torch. However, PyTorch specially designed for Python. Pytorch have pre-defined modules (Linear layer, Convolution layer) like Torch. However, Pytorch does not require to pack model into container. In Pytorch networks are defined in forward-pass thanks to Dynamic Neural Networks feature. Therefore, user can use Python control flow to define a network. For example, one can use for loop to run recurrent neural network (see listing \ref{lst:pytorchrnn}) .
This feature allows us to implement Recursive Neural Network for NLP, which the network change for every sample, much more easier. 

\begin{lstlisting}[caption={RNN},label={lst:pytorchrnn}, language={python}]
import torch
import torch.nn as nn
rnn = nn.RNNCell(10, 20)
seq_len = 10
input_dim = 100
hidden_dim = 150
input = Variable(torch.randn(seq_len, 1, input_dim))
hx = Variable(torch.zeros(1, hidden_dim))
output = []
for i in range(6):
    hx = rnn(input[i], hx)
    output.append(hx)
\end{lstlisting}

We also implement treelstm from original Torch7\footnote{\url{https://github.com/stanfordnlp/treelstm}} sentiment classification task in PyTorch and publish on Github\footnote{\url{https://github.com/ttpro1995/TreeLSTMSentiment}}.

We choose PyTorch because:
\begin{itemize}
    \item Dynamic Neural Networks feature works well on data sequence with different length
    \item Good documentation
    \item Intuitive framework
    \item Easy to install and run on CUDA
\end{itemize}
