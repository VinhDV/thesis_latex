\hypertarget{chap:intro}{\chapter{Introduction}}
\section{Overview}
\subsection{Sentiment Analysis and its applications}
\subsubsection{Definition}
In the nutshell, sentiment analysis is to determine whether the opinion about a specific product, event, organization is positive or negative. It is also known as opinion mining due to the sentiment was derived from opinions of speaker.

The main objective of sentiment analysis is given document $d$ define as quintuples \cite{liu2012sentiment}:  
\[ ( e_{i}, a_{ij}, s_{ijkl}, h_{k}, t_{l} ) \]
 Where:
\begin{itemize}
	\item $e_{i}$: entity i
	\item $a_{ij}$: aspect j of entity i
	\item $h_{k}$: holder k
	\item $t_{l}$: time l
	\item $s_{ijkl}$: opinion of holder k about aspect j of entity i at time l
\end{itemize}


\subsubsection{Different levels in sentiment analysis}
\paragraph{Document-level}
Document-level sentiment analysis is a text classification problem. The system was given a document $d$ and classifies whether the overall opinion was positive, negative. The document can be a reviews of a movie. For example,~\cite{pang2002thumbs} perform document-level sentiment analysis on IMDB movies review data. Document-level sentiment analysis assume whole document give opinion toward one entity and does not apply to document contain multiple entity ~\cite{liu2012sentiment}.
 
\paragraph{Sentence-level}\label{sec:sent-level}
Sentence-level sentiment analysis is to determine whether a sentence expressed positive or negative. Similar to document-level sentiment analysis, a sentence is assume only contain opinion toward one entity (a single movie) ~\cite{liu2012sentiment}.
In this thesis, we focus our study on sentence-level sentiment analysis.

This thesis mainly concerns with the problem of sentence-level sentiment analysis. 
Given a sentence, the system classifies where the sentences to two class positive, negative, or five class very negative, negative, neutral, positive, very positive.
The number of classes of sentiment can vary depending on data sets or settings~\cite{Rotten-Tomato}~\cite{socher2013recursive}.

There are only two popular datasets for this task: Rotten Tomatoes Movie Review~\cite{Rotten-Tomato} and Stanford Sentiment Treebank~\cite{socher2013recursive} which was build based on the former.
For more details, the validation and the test set of the two datasets are similar, the differences only appear in training set.
Whereas Stanford Sentiment Treebank provides phrase-level labels, there are only sentence-level labels in Rotten Tomatoes Movie Review~\cite{socher2013recursive}.
Since the publication of Stanford Sentiment Treebank dataset in 2013, the dataset has been widely used in most researches as a replacement for Rotten Tomatoes Movie Review dataset~\cite{treeLSTM}~\cite{KimCNN}~\cite{cnn-rnn}~\cite{2-layer-cnn}~\cite{socher2013recursive}.
For the purpose of comparison, all of our models in this thesis were evaluated using Stanford Sentiment Treebank.

\paragraph{Entity and Aspect-level}
Entity and Aspect-level sentiment analysis's purpose is to determine opinion, whether positive or negative, against specific aspect of entity (the opinion target)~\cite{liu2012sentiment}. The system look at document $d$ and find sentiment $s_{ij}$ toward aspect $a_{j}$ of entity $e_{i}$.

For example, the reviews \textit{"Loved the book, Severely disappointed with movie"} \footnote{Part of review on Ender's Game IMDB \url{http://www.imdb.com/title/tt1731141/reviews}} target both the book and a movie translated from the book of same name. The overall star on IMDB was 3 out of 10, which is negative. However, their are two entity in the reviews, the book and the movies. In human level respective, we can see that the book gets positive opinion (\textit{Loved the book}) and the movie get negative opinion (\textit{Severely disappointed with movie}). 

 Furthermore, of giving opinion for each entity, a opinion can compare two entity with similar aspect \cite{jindal2006mining}. For example, \textit{"Macbook is expensive than Dell"} compare prices of two entity \textit{"Macbook"} and \textit{"Dell"}. 

\subsubsection{Applications}
Before a customer buys any product, the decision of whether or not he will buy it depends largely on his prior opinions about that product. 
These opinions, in turn, have been built based on his opinions on relating companies or products and other customers' opinions about that product.  
After having been experiencing the product, his posterior opinions on it not only tell us about which features he like or dissatisfy with.
They also inform us about the reasons why he bought it, his expectations, needs and even more detailed information about him.
In a circle, his opinions will also affect the opinion of new customers and even the design of future products.
As a result, customers' opinions are the controllers behind every companies' good decisions. 
They shape companies' marketing strategies, policies, and designs of products.
They judge which company is more competent than another.

A long time ago, when companies needed to know opinions of their customers, they conducted surveys, opinion polls and focus groups~\cite{liu2012sentiment}. 
In recent years, thanks to the dramatic growth of social media, customers' opinions are expressed in the highest speed and volume ever recorded in history.
With this amount of data, it is inefficient to read and analyze or even collect them manually. Sentiment analysis offer a way to collect and process public opinion automatically. 

For instance, a cinema wants to know whether people like a City of Ember. The star on IMDB \footnote{http://www.imdb.com/title/tt0970411/} give a overall. However, it star of each reviews do not contain any more information whether the story was good but the actor was bad, or the viewer disappoint because the screenwriter failed to translate the film from the book of same name. In this scenario, it require sentiment analysis to extract the entity (City of Ember movies, City of Ember books), aspect (plot, story, actor, costume...), holder (age, gender, nationality of viewer), time (just release on Cinema, after release a while when DVD available). The data helps cinema choose movies and marketing strategy to target customer at their area.



\subsection{Deep Learning in Sentiment Analysis}
In recent years, the advancements of deep learning have led to dramatic improvements in the field of Sentiment Analysis:
\begin{description}
\item [2013] Based on the theory that learning appropriate intermediate representations can lead to better generalisation~\cite{knowledge-matter}~\cite{tran-auto-encoder}, Socher and the co-authors augmented Rotten Tomatoes Movie Review  (RT-MR) dataset~\cite{Rotten-Tomato} with phrase-level sentiment labels (the new dataset was named \hyperref[sec:sst]{Stanford Sentiment Treebank} (SST)), in this way, any network can learn to correctly classify phrase-level sentiment, before it learns to classify sentence-level sentiment~\cite{socher2013recursive}. Along with the dataset, the authors also introduced three Recursive Neural Network, which inspired by the recursive structure of language ~\cite{socher2013recursive}.
They were able to archive state of the art performance on SST test set (which is similar to RT-MR test set). 
Since then, this dataset has become the most popular dataset for the task of sentence-level sentiment analysis.

In the same year, two other studies which have a large impact on the whole NLP community appeared. 
Mikolov and his partners introduced word2vec which has the ability to learn a certain level of syntactic and semantic relationships among words~\cite{word2vec}. 
Pre-learned word presentations have been used widely and become the “secret sauce” for the success of recent NLP systems~\cite{Luong_betterword}.

Another research~\cite{GravesLSTM} popularized \hyperref[sec:lstm]{Long Short Term Memory Network (LSTM)} and its "staked" variations (e.g. \hyperref[sec:multilayer-lstm]{multilayer LSTM} , \hyperref[sec:bilstm]{Bidirectional LSTM}).
By mitigating the problem of \hyperref[sec:gradient-vanish]{gradient vanishing} of \hyperref[sec:RNN]{RNNs}, LSTMs and its variations become so popular that, we can find them in most NLP publications from 2014 to 2017. 
LSTMs is one of the first successful variation of RNNs which leads the way to many other more advanced RNNs variation~\cite{olah2016attention}.

\item [2014] Applying only \hyperref[kim-cnn]{one layer CNN on multi-channel word embeddings}, Yoon Kim successfully archived state of the art performance on \hyperref[sec:sst]{SST (binary setting)} as well as other datasets of different tasks~\cite{KimCNN}. 
This success attracted many future researches applying CNN on sentiment analysis~\cite{2-layer-cnn}~\cite{cnn-rnn}.

Another research which we applied in our thesis is Glove method for training word embeddings~\cite{glove}.
Different from word2vec, Glove vectors captures word co-occurrences globally in the corpus, whereas word2vec only captures local word co-occurrences in each window of its training examples~\cite{glove}.

\item [2015] Combining recursive structure of Recursive Neural Networks~\cite{socher2013recursive} and LSTM~\cite{originLSTM}, Kai Sheng Tai, Socher and Christopher Manning introduced \hyperref[sec:treelstm]{tree-structured LSTM}  (TreeLSTM)~\cite{treeLSTM}.
Their models archived state of the art performance on \hyperref[sec:sst]{SST (fine-grained setting)} and task of semantic relatedness (SemEval~2014, Task~1~\cite{SemeEvalTask1}).
This success lead to multiple researches~\cite{need-tree}~\cite{bowman-treevslstm}~\cite{Graves_Nature2016} which aim to \hyperref[treelstm-advantage]{comparing tree-structured and sequential LSTM}. 
The question is whether tree-structured networks are necessary or at least have some advantages over sequential architects when processing recursive-structured languages~\cite{need-tree}~\cite{bowman-treevslstm}.   

In the same year, there are two researches~\cite{ParagraphVec}~\cite{semisup-seq2seq} of Quoc V. Le which introduced several methods for \hyperref[sec:unsupervised-pretrain]{unsupervised pre-training neural network models for NLP tasks}.
These methods help network models to mitigate over-fitting (which lead to better generalization) by allowing them to be pre-trained on large unlabeled datasets.
We will apply several unsupervised pre-train methods in this thesis.

\item [2016] Xingyou Wang and his partners combined convolutional and recurrent neural networks~\hyperref[cnn-rnn]{(CNN-RNN)} to archive \hyperref[table:cnn-rnn]{staggering improvement} on SST (both binary and fine-grained setting).
Until now\footnote{July, 2017}, their models are state of the art on SST and also RT-MR~\cite{cnn-rnn}.
\end{description}  

\section{Bridging the gaps}
\subsection{Utilizing local syntactic information at each node of Recursive Neural Networks}
\subsubsection{Observation}
In most cases, to understand a sentence, we have to understand the phrases composing it.
Analogously, to understand a phrase, we have to understand the phrases and words composing it.
Recursive Neural Networks were mainly inspired by this idea~\cite{treeLSTM}.
Given a sentence and its parse tree, a Recursive Neural Network composes the vector presentation of the sentence by apply it composition function at each node of the parse tree in a bottom-up manner.
For demonstration, parse tree of the phrase ``is very interesting'' and the it composing process using a Recursive Neural Network are illustrated in Fig.\ref{fig:example-parse}~\cite{tag-embedding-rnn} and Fig.\ref{fig:example-compose} respectively.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figure/example-parse}
    \caption[Constituency parse tree for the phrase ``is very interesting'']{Constituency parse tree for the phrase ``is very interesting''.}
    \label{fig:example-parse}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.46]{figure/example-compose}
    \caption[Applying Recursive Neural Network on the phrase ``is very interesting'']{Applying Recursive Neural Network on the Constituency parse tree of the phrase ``is very interesting''. 
    The composition function of this network is denoted as \textbf{g}.}
    \label{fig:example-compose}
\end{figure}

Recursive Neural Networks are linguistically attractive models due to their ability to compose sentences with respect to their parse trees~\cite{treeLSTM}. 
Although having this ability, most Recursive Neural Networks (e.g. RNN, MV-RNN, RNTN~\cite{socher2013recursive}, DRNN~\cite{IrsoyDRNN}) do not explicitly utilize to local syntactic information at each parse tree's node.
By incorporating the local syntactic information into the composition function at each parse tree's node, TE-RNTN~\cite{tag-embedding-rnn} was able to not only dramatically improve the performance of RNTN (Socher, 2013)~\cite{socher2013recursive} (from \(85.4\%\) to \(87.7\%\) accuracy) but also their models used much smaller number of parameters \((54K)\) compared to that of RNTN \((108K)\)~\cite{tag-embedding-rnn}.
Composing process of the Constituency parse tree of the phrase ``is very interesting'' using TE-RNTN is illustrated in Fig.\ref{fig:example-compose-tag}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figure/example-compose-tag}
    \caption[Applying TE-RNTN on the phrase ``is very interesting'']{Applying TE-RNTN~\cite{tag-embedding-rnn} on the Constituency parse tree of the phrase ``is very interesting''. 
    The composition function of this network is denoted as \textbf{g}.
    To compose vector presentation of a phrase, composition function \textbf{g} takes vector presentations of: its constituting phrases (words); categories of the grammar of those phrases (words).}
    \label{fig:example-compose-tag}
\end{figure}

Shortly after the publication of TE-RNTN, Tree-LSTMs~\cite{treeLSTM} were introduced.
The core idea behind the design of Tree-LSTMs is to generalize the LSTMs so that tree-structured inputs.
Tree-LSTMs were able to archive state-of-the-art performance on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1~\cite{SemeEvalTask1}) and sentiment classification (Stanford Sentiment Treebank~\cite{socher2013recursive}).
Despite thier successes, similar to RNTN~\cite{socher2013recursive}, Tree-LSTMs do not explicitly utilize to local syntactic information at each parse tree's node.

\subsubsection{Hypothesis}
Given the success of TE-RNTN, we hypothesized that by parameterized the composition functions of a Tree-LSTMs unit at a node with respect to the grammar rule expanding that node, we can improve the performance of Tree-LSTMs (in the same way how this method improving RNTN).

\subsubsection{Experiment}
We did experiments on several variations of Tree-LSTMs which incorporating different types local syntactic features.
These features includes Constituency grammar rule, Dependency relation, part-of-speech tag and a combination of both Dependency relation, part-of-speech tag.

One of our models was able to outperformed Dependency Tree-LSTM when using the same dependency parsed dataset. 
But disappointedly, TE Tree-GRU underperformed Dependency Tree-LSTM on task of Semantic Relatedness (SemEval 2014, Task 1~\cite{SemeEvalTask1}) which pointed out the strength of Dependency Tree-LSTM and the main reason why it was presented in the original paper~\cite{treeLSTM}.
\textbf{In general, this approach was failed to improve Tree-LSTMs}, so we tried two more different approaches.

\subsection{Transfer Learning by retraining Glove on Amazon Reviews dataset}
\subsubsection{Observation}
There are many words (e.g. ``B-rated'', ``Batman'', ``Nolan'', ``cartoonlike'') which rarely appears in regular documents but more often in movie reviews.
Additionally, the ways people use words in movie reviews might different from their usage in general documents.
For example the vector presentations of ``sympathy'' and ``disappointed'', or ``boom'' and ``insult'' are very close to each other in Glove Common Crawl\footnote{Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) publicly available at \url{https://nlp.stanford.edu/projects/glove/}} but if we have to predict the sentiment of a movie comment which has one of these word, they should be distinctive.

Noticeably, many experiments~\cite{treeLSTM}~\cite{KimCNN} have shown that modifying word embeddings during the training process help improving performance of Deep Learning systems.
Although this method helps to improve models' performance, it can harm generalization by updating only words which appear in the training set and not related words which only appear in test set.
We demonstrates a case in which this method harm the generalization ability of a classifier in Fig.\ref{fig:updating-word-bad-1} and Fig.\ref{fig:updating-word-bad-2}\footnote{Source: \url{https://cs224d.stanford.edu/lectures/CS224d-Lecture4.pdf}}.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{figure/vec-before-update}
    \caption[Word embeddings updating causing over-fitting 1]{Vector presentations of words before being updated by the training process. 
    Note that ``telly'', ``TV'' and ``television'' are still close to each other before being updated. 
    The words ``telly'' and ``TV'' appear in the training set, but ``television'' does not.
    The green dot at the bottom right corner belong to training set, so the training process will "try" to fix this by updating vector presentations of ``telly'' and ``TV''.
     }
    \label{fig:updating-word-bad-1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.31]{figure/vec-after-update}
    \caption[Word embeddings updating causing over-fitting 2]{Vector presentations of words after being updated by the training process.
    In the test set, the trained classifier is likely to wrongly classify the examples which contain the word ``television''.}
    \label{fig:updating-word-bad-2}
\end{figure}

We also observe that the available amount of document-level labeled sentiment data (e.g. Amazon Reviews dataset~\cite{amazon-reviews} has 83.68 million reviews) is gigantic compared to the amount of sentence or phrase-level sentiment data (e.g. Stanford Sentiment Treebank~\cite{socher2013recursive} has 8544 sentences in its training set, even it is the biggest sentence-level sentiment analysis dataset).
Although, there are some attempts to do transfer learning~\cite{group-instance}~\cite{re-embedding} (i.e. training on large document-level sentiment dataset then fine-tuning and/or test on sentence-level sentiment dataset), their performances are not high when evaluated on Stanford Sentiment Treebank~\cite{group-instance}.

\subsubsection{Hypothesis}
We utilized Glove method to do transfer learning.
We hypothesized that by training Glove on review documents, especially movie or book reviews, we can capture more rare words and also the different way people use words (or different word relationships) to express their opinions on movies or books.
This might help our models archiving better generalisation when training on small sentence-level sentiment dataset like Stanford Sentiment Treebank. 

\subsubsection{Experiment}
For archiving these purposes, we retrained Glove vectors~\cite{glove} on part of  \hyperref[sec:amazon]{Amazon Reviews dataset}.
This new Glove vectors was named Glove Amazon.
For evaluating Glove Amazon, we replaced the Glove Common Crawl\footnote{Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) publicly available at \url{https://nlp.stanford.edu/projects/glove/}} with Glove Amazon for initializing Tree-LSTMs' word embedding layer.

\textbf{In spite of being a simple method, it dramatically improves Tree-LSTMs performance.}
This is our first successful method for transfer learning from document-level (Amazon Reviews) to sentence-level labeled dataset (Stanford Sentiment Treebank).
Inspired by this method, we cooperated it into the development of our next approach.

\subsection{Combining Recusive Neural Networks with Convolution Neural Networks}

\subsubsection{Observation}
Based on our discussion on tree-structured versus sequential network architects (Sec.\ref{sec:tree-discuss}) and the benefits of using convolution layer (Sec.\ref{kim-cnn}), we combined Convolution Neural Networks with Tree-LSTM and \hyperref[sec:lstm]{sequential LSTM}.

\subsubsection{Hypothesis}
We hypothesized that the convolution layer will help Tree-LSTMs to mitigate the problem of lacking local context and weak feature capturing at leaf nodes (Sec.\ref{sec:tree-discuss}).
Additionally, using Tree-LSTM to combine the feature maps produced by convolution layer is better than max-over-time pooling layer (Sec.\ref{kim-drawback}).
The increased model complexity can lead to over-fitting.
We tackled this risk by unsupervised pre-training the models on the large Amazon Reviews dataset using methods described in Sec.\ref{sec:unsupervised-pretrain}.
Based on the success of Glove Amazon, we expected that the unsupervised pre-training process (on Amazon Reviews) help the models to capture not only generic language features but also specific knowledge about Film industry and human culture (Sec.\ref{sec:nlm}).

\subsubsection{Experiment}
With this approach, \textbf{we was able to archive state-of-art\footnote{July, 2017} performance on Stanford Sentiment Treebank}.


\section{Contributions}
In search of new improvements on the task of sentence-level sentiment analysis, we have tried three approaches: Utilizing local syntactic information at each node of Recursive Neural Networks; Transfer Learning by retraining Glove on Amazon Reviews dataset and Combining Recursive Neural Networks with Convolution Neural Networks.
Based on our results in Table.\ref{table:experimentresult}, \textbf{hypotheses that supported by our results including}:
\begin{itemize}
\item Amazon Glove captured some good\footnote{good for the task of sentiment analysis of movie reviews} features that dose not exist or hardly be extracted in Glove Common Crawl. (Sec.\ref{proved:Amazon-adv-Common})

\item There are some good features dose not appear in Glove Amazon but only appear in Glove Common Crawl or when combining both Glove Amazon and Glove Common Crawl. (Sec.\ref{proved:Common-syn-Amazon})

\item By adding a convolution layer before the leaf-node of Tree-LSTM, convolution layer will help Tree-LSTMss to mitigate the problem of lacking local context and weak feature capturing at leaf nodes (Sec.\ref{sec:tree-discuss}).
Mutually, using Tree-LSTM to combine the feature maps produced by convolution layer is better than max-over-time pooling layer (Sec.\ref{kim-drawback}). (Sec.\ref{proved:tree-conv-benefit})

\item  Tree-LSTMs have already utilized the information in word embeddings and the local syntactic information from tag embeddings adding no more value. (Sec.\ref{unproved:tag-useless})
\end{itemize}

\section{Structure of this thesis}
\begin{description}
\item [\deschyperlink{chap:background}{Chapter 2}] introduces some basic knowledge in NLP, Deep Learning and programming framework for implementing Deep Learning systems. 
\item [\deschyperlink{chap:related}{Chapter 3}] builds up the theoretical framework for the whole thesis. 
It analyzes the closely related works -- all the models or methods presented here are being used by at least one of our models.
For clarity, we divide Chapter 3 into two parts.
The first part is \hyperref[sec:dataset]{"Datasets"}. 
This part contains information about all datasets which were used in this thesis.
The second part is \hyperref[sec:composer]{"Neural network architects for sentence composition"}.
The models presented in this part are used to compose vector presentations of sentences.
These presentation vectors are then used to classify the sentiment class of the sentence.
For each model, we describe its structure, training method and evaluation on SST.
Additionally, we analyze advantages and disadvantages of each model compared to the others.
In this part, we also present and discuss several \hyperref[sec:unsupervised-pretrain]{unsupervised pre-train methods}.
\item [\deschyperlink{chap:method}{Chapter 4}] describes our new network architects, the reasons behind their designs, their training methods and other experiments. 
\item [\deschyperlink{chap:result}{Chapter 5}] presents, analyzes and discusses the empirical comparisons of our models and other related models.
\item [\deschyperlink{chap:conclude}{Chapter 6}] summarizes the achieved results, describes future works and conclusion.
\end{description}